{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf706d63-e6e6-44d8-9b39-49c869f4b15f",
   "metadata": {},
   "source": [
    "추천 시스템에 사용 될 데이터 크롤링 및 전처리 작업\n",
    "\n",
    "크롤링 및 전처리를 위한 환경설정\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1860bf7a-0e93-4848-87d2-776878969549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openpyxl\n",
      "  Using cached openpyxl-3.1.5-py2.py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (2.31.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (4.12.2)\n",
      "Requirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (2.1.0)\n",
      "Collecting et-xmlfile (from openpyxl)\n",
      "  Using cached et_xmlfile-2.0.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests) (2023.7.22)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from beautifulsoup4) (2.4.1)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas) (1.24.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Using cached openpyxl-3.1.5-py2.py3-none-any.whl (250 kB)\n",
      "Using cached et_xmlfile-2.0.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: et-xmlfile, openpyxl\n",
      "Successfully installed et-xmlfile-2.0.0 openpyxl-3.1.5\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install openpyxl requests beautifulsoup4 pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82b9f83f-2d2b-48ec-86d0-085f86d6d0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openpyxl # excel 사용\n",
    "import requests # 주소 긁어오기\n",
    "from bs4 import BeautifulSoup # 이쁘게 다듬기 \n",
    "import time # 작업 중간에 시간 체크\n",
    "import pandas as pd \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') # 모든 경고 메시지를 무시해라"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b4e2a4-dac6-4a1c-bbcf-8beec8205947",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b60d0059-9a5b-43a7-a114-fb8b55e0f64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "webpage = requests.get('https://www.thinkcontest.com/Contest/ContestDetail.html?id=25589') #웹페이지 요청 후\n",
    "soup = BeautifulSoup(webpage.content, \"html.parser\") \n",
    "# BeautifulSoup을 사용해 webpage.content를 구조화. [HTML 요소(태그, 속성, 텍스트 등)에 접근]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0eaf02-ee42-4b10-bdaf-5a4c97725432",
   "metadata": {},
   "source": [
    "## 시상금액 규모별 공모전 정보     add : address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "752d1444-2bcb-4082-a427-539e701d3ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawling_tg(add):\n",
    "    webpage = requests.get(add)                          # 해당 주소로 GET 요청을 보내고 HTML 응답 데이터를 가져옴.\n",
    "    soup = BeautifulSoup(webpage.content, \"html.parser\") # HTML 문서를 파싱(구문 분석)하여 Python 객체로 변환하는 역할\n",
    "    men = soup.find_all(attrs={'class':\"txt-left\"})      # 필요한 공모전 정보를 추출하기 위해 클래스명이 txt-left인 HTML 요소들을 모두 찾아 리스트로 저장.\n",
    "    wb = openpyxl.Workbook()                             # 새로운 엑셀파일 생성\n",
    "    sheet = wb.active                                    # 현쟈 작업중인 시트를 가리켜라. \n",
    "    sheet.append(['공모명','분류','기간','조회수'])            # 첫번쨰 행에 제목 행 추가\n",
    "    #데이터를 각각  저장하기 위해 리스트 초기화\n",
    "    l1=[]\n",
    "    l2=[]\n",
    "    l3=[]\n",
    "    l4=[]\n",
    "\n",
    "\n",
    "    # 데이터 크롤링 (공모명, 분류)\n",
    "    for i in men:                                            # txt-left 클래스를 가진 각 요소에서 필요한 정보를 추출.\n",
    "        l1.append(i.select_one('a').get_text())              # a 태그 내부의 텍스트(공모전 이름)를 가져와 l1에 추가.\n",
    "        l2.append(i.select_one('.contest-cate').get_text())  # 클래스명이 contest-cate인 요소의 텍스트(공모전 분류)를 가져와 l2에 추가.\n",
    "\n",
    "    # 데이터 크롤링 (기간, 조회수)\n",
    "    for i in soup.find_all('td'):                            # 모든 <td> 태그를 순회하며 기간과 조회수를 추출.\n",
    "        # 기간 데이터\n",
    "        if '~' in i.text:\n",
    "            l3.append(i.get_text())                          # if '~' in i.text: 텍스트에 ~가 포함된 경우 기간으로 간주하고 l3에 추가.\n",
    "        # 조회수 데이터\n",
    "        if (i.text.isdigit()):                               # 숫자만 포함된 텍스트를 조회수로 간주하고 l4에 추가.\n",
    "            l4.append(i.get_text())\n",
    "   \n",
    "    # 데이터 매핑 및 엑셀 파일 저장    [[공모명1, 분류1, 기간1, 조회수1], [공모명2, ...]] 형태.\n",
    "    men_list=list(map(lambda x1,x2,x3,x4:[x1,x2,x3,x4], l1,l2,l3,l4))   # map 함수와 lambda를 사용해 4개의 리스트(l1, l2, l3, l4)의 \n",
    "                                                                        # 각 요소를 병합하여 리스트의 리스트를 만듦.\n",
    "    for i in men_list:\n",
    "        sheet.append(i)\n",
    "    \n",
    "    wb.save('./data/money/data'+str(add)[-1]+'.xlsx')                   # <마지막 숫자>는 add 주소 문자열의 마지막 문자를 사용하여 파일 이름을 생성."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cccc50de-2c49-44ed-9949-28288039840b",
   "metadata": {},
   "source": [
    "# 전체 작업 흐름\n",
    "1. 각 페이지에서 크롤링할 데이터를 수집합니다.\n",
    "2. 데이터를 엑셀 파일에 저장합니다.\n",
    "3. 10페이지마다 남은 시간을 출력하여 크롤링 진행 상황을 알립니다.\n",
    "4. 모든 페이지를 크롤링한 후, 엑셀 파일에 데이터 저장을 완료합니다\n",
    "\n",
    "#### 이 코드는 주어진 페이지 수(p)와 카테고리(c)에 대해 각 페이지를 크롤링하고, 추출된 데이터를 엑셀 파일로 저장하는 기능을 제공합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c8055b3-efd2-4320-8485-03fadbc8ed1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def craw(p, c):\n",
    "    sheet_space=[[] for i in range(p)] #각 페이지에서 크롤링된 데이터를 저장할 리스트. p만큼 크기를 갖는 빈 리스트를 초기화\n",
    "    \n",
    "    # 페이지 반복 (크롤링)\n",
    "    for k in range(1, p+1):            # k는 크롤링할 페이지 번호\n",
    "        st_time =time.time()           # \n",
    "\n",
    "        webpage = requests.get('https://www.thinkcontest.com/Contest/CateReward.html?page='+str(k)+'&c='+str(c)) # HTML 가져오기\n",
    "        soup = BeautifulSoup(webpage.content, \"html.parser\") # BeautifulSoup로 HTML을 파싱하여 데이터를 추출할 준비\n",
    "        \n",
    "        #데이터 추출\n",
    "        men = soup.find_all(attrs={'class':\"txt-left\"})            # class=\"txt-left\" 속성을 가진 HTML 요소들을 모두 찾아 men 리스트에 저장\n",
    "        wb = openpyxl.Workbook()                                   # 엑셀 파일 생성\n",
    "        sheet = wb.active\n",
    "        sheet.append(['공모명','분류','기간','조회수'])                 # 제목 행 추가 [공모명, 분류, 기간, 조회수]\n",
    "        l1=[]\n",
    "        l2=[]\n",
    "        l3=[]\n",
    "        l4=[]\n",
    "\n",
    "        # 공모명과 분류 데이터 추출\n",
    "        for i in men:\n",
    "            l1.append(i.select_one('a').get_text())                # 공모명: a 태그 내의 텍스트를 추출하여 l1 리스트에 추가.\n",
    "            l2.append(i.select_one('.contest-cate').get_text())    # 분류: 클래스명이 contest-cate인 요소의 텍스트를 추출하여 l2 리스트에 추가.\n",
    "\n",
    "        # 기간 및 조회수 데이터 추출\n",
    "        for i in soup.find_all('td'):      # <td> 태그에서 데이터 추출\n",
    "            if '~' in i.text:                                      # 기간: 텍스트에 ~ 문자가 포함된 경우 이를 기간으로 판단하고 l3 리스트에 추가.\n",
    "                l3.append(i.get_text())\n",
    "\n",
    "            if (i.text.isdigit()):                                 #  조회수: 텍스트가 숫자인 경우 이를 조회수로 판단하고 l4 리스트에 추가.\n",
    "                l4.append(i.get_text())\n",
    "\n",
    "        # 크롤링된 데이터를 결합  : 각 공모전의 공모명, 분류, 기간, 조회수가 하나의 리스트로 묶여 men_list에 저장\n",
    "        men_list=list(map(lambda x1,x2,x3,x4:[x1,x2,x3,x4], l1,l2,l3,l4))\n",
    "\n",
    "        # 페이지별 데이터 저장 : 각 페이지에서 크롤링한 데이터를 sheet_space[k-1]에 저장\n",
    "        for men in men_list:\n",
    "            sheet_space[k-1].append(men)   # sheet_space는 페이지별로 데이터를 저장하는 리스트\n",
    "\n",
    "        # 진행 상태 출력   : 페이지가 10개 단위로 진행될 때마다 작업 시간을 측정하여 남은 시간과 함께 진행 상태를 출력\n",
    "        if k%10==0:\n",
    "            ed_time =time.time()-st_time   #ed_time은 현재 페이지까지의 소요 시간을 측정하고, 이를 기반으로 남은 시간을 계산합니다.\n",
    "            print(\"\\r남은 개수 :\",k,\"/\",p,\"\\t\\t남은 시간 :\",round(ed_time*(p-k-1),2),\"\\t\\t\",end='')\n",
    "\n",
    "    \n",
    "    # 모든 데이터를 엑셀 파일에 저장\n",
    "    for r in range(p):\n",
    "        for q in range(len(sheet_space[r])):\n",
    "            sheet.append(sheet_space[r][q])\n",
    "\n",
    "    wb.save('./data/money/data'+str(c)+'.xlsx')\n",
    "    \n",
    "# 함수 호출 예시\n",
    "#craw(47, 1)  # 47 페이지, 카테고리 1의 데이터를 크롤링\n",
    "#craw(41, 2)  # 41 페이지, 카테고리 2의 데이터를 크롤링\n",
    "#craw(256, 3)  # 256 페이지, 카테고리 3의 데이터를 크롤링\n",
    "#craw(811, 4)  # 811 페이지, 카테고리 4의 데이터를 크롤링"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77e21c4-8f67-4b5e-ad2f-60316dcef11a",
   "metadata": {},
   "source": [
    "## 이미지 링크와 공모전 제목을 추출해 엑셀 파일로 저장하는 함수 : get_image\n",
    "### 코드 흐름 요약\n",
    "1. 주어진 p까지 반복하며 각 공모전의 상세 페이지를 크롤링합니다.\n",
    "2. 각 페이지에서 공모전 제목과 이미지 URL을 추출합니다.\n",
    "3. 추출한 데이터를 엑셀 시트에 저장합니다.\n",
    "4. 50페이지마다 진행 상태를 출력하여 남은 시간 등을 예측합니다.\n",
    "5. 예외가 발생하면 해당 페이지 ID를 출력하고 크롤링을 계속 진행합니다.\n",
    "6. 모든 크롤링이 완료되면 엑셀 파일로 저장됩니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c60f29d1-e624-40e0-a63d-5fe10a74af6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image(p):\n",
    "    wb = openpyxl.Workbook()            # 엑셀파일 생성\n",
    "    sheet = wb.active                   # 생성파일에 들어가서\n",
    "    sheet.append(['공모전명', '이미지'])    # 첫 시트에 제목 행(공모전명, 이미지))을 추가\n",
    "    \n",
    "    try:\n",
    "        # 페이지 크롤링\n",
    "        for i in range(1, p):           # i는 크롤링할 공모전의 ID, 1번부터 p-1까지 반복하여 각 공모전의 상세 페이지를 가져오기\n",
    "                st_time =time.time()\n",
    "                webpage = requests.get('https://www.thinkcontest.com/Contest/ContestDetail.html?id='+str(i))\n",
    "                soup = BeautifulSoup(webpage.content, \"html.parser\")\n",
    "\n",
    "                title_list = [] \n",
    "                image_list = []\n",
    "                text_list = []\n",
    "\n",
    "                #제목 및 이미지 링크 추출\n",
    "                soup_title = soup.find('title').get_text().replace('씽굿_전체공모전현황_공모전 상세보기_','') # HTML 페이지의 <title> 태그에서 공모전의 제목을 가져옵니다. \n",
    "                                                                                                     # 제목에서 불필요한 부분 (씽굿_전체공모전현황_공모전 상세보기_)을 제거.\n",
    "                image_link = soup.head.find('meta', {'property':'og:image'}).get('content')          # <meta> 태그에서 og:image 속성을 찾아 공모전 이미지의 URL을 추출합니다.\n",
    "                \n",
    "#                 if soup.find_all(attrs={'class':\"info-cont\"}) == []: \n",
    "#                     soup_text = 'error'\n",
    "                \n",
    "#                 else :\n",
    "#                     soup_text = soup.find_all(attrs={'class':\"info-cont\"})[0].get_text()\n",
    "               \n",
    "                # 제목 및 이미지 리스트에 저장\n",
    "                title_list.append(soup_title) \n",
    "                image_list.append(image_link)\n",
    "#                 text_list.append(soup_text)\n",
    "\n",
    "                # 진행 상태 출력    : i가 50의 배수일 때마다 작업 진행 상태를 출력\n",
    "                if i%50 == 0:\n",
    "                    ed_time =time.time()-st_time\n",
    "                    print(\"\\r남은 개수 :\",i,\"/\",p,\"\\t\\t남은 시간 :\",round(ed_time*(p-i-1),2),\"\\t\\t\",end='')\n",
    "\n",
    "                sheet.append([title_list[0], image_list[0]])\n",
    "            \n",
    "    # 오류가 발생할 경우, 예외가 발생한 공모전 ID (i)를 출력\n",
    "    except :\n",
    "        print(i)\n",
    "    \n",
    "    wb.save('./data/get_image'+str(p)+'.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89702e18-cd1b-4711-99fa-63dc5da17248",
   "metadata": {},
   "source": [
    "## 공모전 목록과 관련 정보를 관리\n",
    "### 코드 흐름 요약\n",
    "1. 주어진 p까지 반복하며 각 공모전의 상세 페이지를 크롤링합니다.\n",
    "2. 각 페이지에서 공모전 제목과 내용을 추출합니다.\n",
    "3. 추출한 데이터를 엑셀 시트에 추가합니다.\n",
    "4. 10페이지마다 진행 상태를 출력하여 남은 시간 등을 예측합니다.\n",
    "5. 예외가 발생하면 해당 페이지 ID를 건너뛰고 크롤링을 계속 진행합니다.\n",
    "6. 모든 크롤링이 완료되면 엑셀 파일로 저장됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd6cfbdf-3e6e-4708-a5a3-6f70c39f8c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text(p):\n",
    "    wb = openpyxl.Workbook()      # p: 크롤링할 공모전 ID의 범위 (예: 1번부터 p번까지).\n",
    "    sheet = wb.active             # 엑셀 워크북을 생성하고, 첫 번째 시트에 제목(공모전명)과 내용을 저장할 헤더 행을 추가\n",
    "    sheet.append(['공모전명', '내용'])\n",
    "    \n",
    "    for i in range(1, p):\n",
    "        try:\n",
    "            # 페이지 크롤링\n",
    "            st_time =time.time()\n",
    "            webpage = requests.get('https://www.thinkcontest.com/Contest/ContestDetail.html?id='+str(i))\n",
    "            soup = BeautifulSoup(webpage.content, \"html.parser\")\n",
    "            \n",
    "            # 추출된 제목과 내용을 각각 저장할 리스트\n",
    "            title_list = []\n",
    "            text_list = []\n",
    "            \n",
    "            # 페이지의 <title> 태그에서 공모전 제목만 추출\n",
    "            soup_title = soup.find('title').get_text().replace('씽굿_전체공모전현황_공모전 상세보기_','')\n",
    "\n",
    "            if len(soup.find_all(attrs={'class':\"info-cont\"})):                                  \n",
    "                soup_text = soup.find_all(attrs={'class':\"info-cont\"})[0].get_text()          # 공모전의 상세 내용은 class=\"info-cont\"를 가진 태그에서 추출\n",
    "            \n",
    "            else :\n",
    "                soup_text = 'error'                                                           # 태그가 없다면 soup_text는 'error'로 설정\n",
    "\n",
    "            title_list.append(soup_title)    \n",
    "            text_list.append(soup_text)\n",
    "\n",
    "            # 작업 진행 상태를 출력\n",
    "            if i%10 == 0:\n",
    "                ed_time =time.time()-st_time\n",
    "                print(\"\\r남은 개수 :\",i,\"/\",p,\"\\t\\t남은 시간 :\",round(ed_time*(p-i-1),2),\"\\t\\t\",end='')\n",
    "            \n",
    "            # 엑셀 시트에 추출한 제목과 내용을 추가\n",
    "            sheet.append([title_list[0], text_list[0]])\n",
    "\n",
    "        except:\n",
    "            pass     # 예외가 발생하면 해당 공모전 ID에 대한 작업을 건너뛰고 진행.\n",
    "\n",
    "    wb.save('./data/get_text'+str(p)+'.xlsx')    # 엑셀 파일 저장 p는 크롤링한 공모전 ID의 수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a80bd4-a010-42c2-901d-373afe0a103f",
   "metadata": {},
   "source": [
    "## 공모전의 제목과 상세 정보를 쉽게 관리하고 분석\n",
    "### 코드 흐름 요약\n",
    "1. 주어진 p까지 반복하여 각 공모전의 상세 페이지를 크롤링합니다.\n",
    "2. 각 페이지에서 공모전 제목, 조회수, 주최, 주관, 응모분야, 참가자격, 접수기간, 1등 시상금 등 상세 정보를 추출합니다.\n",
    "3. 추출한 정보를 엑셀 시트에 한 행씩 추가합니다.\n",
    "4. 10페이지마다 진행 상태를 출력하여 남은 시간 등을 예측합니다.\n",
    "5. 예외가 발생하면 해당 공모전 ID를 건너뛰고 크롤링을 계속 진행합니다.\n",
    "6. 모든 크롤링이 완료되면 엑셀 파일로 저장됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82b1930f-3f96-4c4b-8b2e-918e8e5bc642",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_detail(p):\n",
    "    wb = openpyxl.Workbook()\n",
    "    sheet = wb.active\n",
    "    sheet.append(['공모전명_plus', '조회수', '주최','주관', '응모분야','참가자격','접수기간','1등시상금'])\n",
    "\n",
    "    for i in range(1, p):\n",
    "\n",
    "        try:\n",
    "            st_time = time.time()\n",
    "            webpage = requests.get('https://www.thinkcontest.com/Contest/ContestDetail.html?id='+str(i))\n",
    "            soup = BeautifulSoup(webpage.content, 'html.parser')\n",
    "            # 공모전 상세 정보 추출\n",
    "            # juchae: 공모전의 주최 정보는 첫 번째 <tr> 요소에서 <td> 태그를 찾아 추출\n",
    "            juchae = soup.find_all('tr')[0].find('td').get_text()\n",
    "            for j in range(len(soup.find_all('tr'))):\n",
    "                try :\n",
    "                    #soup.find_all('tr')을 사용하여 모든 <tr> 요소를 찾고, \n",
    "                    #각 행에서 공모전의 다양한 정보를 추출합니다. \n",
    "                    # th 태그를 기준으로 제목을 확인하고, td 태그에서 값을 추출하여 각 변수에 저장\n",
    "                    \n",
    "                    if (soup.find_all('tr')[j].find('th').get_text())=='응모분야':\n",
    "                        join_part = soup.find_all('tr')[j].find('td').get_text()\n",
    "                    elif (soup.find_all('tr')[j].find('th').get_text())=='참가자격':\n",
    "                        join_license = soup.find_all('tr')[j].find('td').get_text()\n",
    "                    elif (soup.find_all('tr')[j].find('th').get_text())=='접수기간':\n",
    "                        join_date = soup.find_all('tr')[j].find('td').get_text()\n",
    "                    elif (soup.find_all('tr')[j].find('th').get_text())=='주관':\n",
    "                        jugan = soup.find_all('tr')[j].find('td').get_text()\n",
    "                    elif (soup.find_all('tr')[j].find('th').get_text())=='1등 시상금':\n",
    "                        join_money = soup.find_all('tr')[j].find('td').get_text()\n",
    "\n",
    "                except :\n",
    "                    pass\n",
    "            # 조회수 및 공모전명 추출\n",
    "            # view: 조회수는 class='views'를 가진 태그에서 텍스트를 추출하여 저장\n",
    "            view = soup.find_all(attrs={'class' : 'views'})[0].get_text()\n",
    "            # title_plus: 공모전명은 class='contest-title'을 가진 태그에서 텍스트를 추출하여 저장\n",
    "            title_plus = soup.find_all(attrs={'class' : 'contest-title'})[0].get_text()\n",
    "\n",
    "            sheet.append([title_plus, view, juchae, jugan, join_part, join_license, join_date, join_money])\n",
    "            \n",
    "            # 진행 상태 출력\n",
    "            if (i%10 == 0) : \n",
    "                ed_time = time.time() - st_time\n",
    "                print(\"\\r남은 개수 :\",i,\"/\",p,\"\\t\\t남은 시간 :\",round(ed_time*(p-i-1),2),\"\\t\\t\",end='')\n",
    "\n",
    "\n",
    "        except :\n",
    "            pass\n",
    "    \n",
    "    wb.save('./data/data_detail'+str(p)+'.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75f0997b-9c72-48cc-b0c3-47614b06d44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## HTML구조 확인하고 태그를 대체한<div class=\"tit\">와 같은게 사용이 될 때,요소에 맞는 새 코드로 작성해야 함. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6f2ba2-2b29-444d-b118-84073ba7a4d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
